{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac9bf21b",
   "metadata": {},
   "source": [
    "# DATA MINING\n",
    "\n",
    "## **WEB SCRAPING TECHNIQUE.** \n",
    "\n",
    "#### **Nombre:** SAMUEL VASCO GONZÁLEZ\n",
    "\n",
    "#### **CC:** 1152223665\n",
    "\n",
    "#### UNIVERSIDAD DE ANTIOQUIA. FÍSICA.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0f88b0",
   "metadata": {},
   "source": [
    "Using web-scraping technique, it save to a csv file all the names and prices of the books from the following website (1-50):\n",
    "https://books.toscrape.com/\n",
    "\n",
    "### Beautifoul Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac5f526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup  #for scrap\n",
    "from urllib.parse import urljoin #for join urls like this, base+href, \n",
    "                                 #urljoin('http://some/more/', 'thing') = 'http://some/more/thing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f661ebf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heaven is for Real: A Little Boy's Astounding Story of His Trip to Heaven and Back 52.86\n",
      "Future Shock (Future Shock #1) 55.65\n",
      "Ender's Game (The Ender Quintet #1) 43.64\n",
      "Diary of a Citizen Scientist: Chasing Tiger Beetles and Other New Ways of Engaging the World 28.41\n",
      "Death by Leisure: A Cautionary Tale 37.51\n",
      "Brilliant Beacons: A History of the American Lighthouse 11.45\n",
      "Brazen: The Courage to Find the You That's Been Hiding 19.22\n",
      "Between the World and Me 56.91\n",
      "Being Mortal: Medicine and What Matters in the End 55.06\n",
      "A Murder Over a Girl: Justice, Gender, Junior High 13.20\n",
      "32 Yolks 53.63\n",
      "\"Most Blessed of the Patriarchs\": Thomas Jefferson and the Empire of the Imagination 44.48\n",
      "You Are a Badass: How to Stop Doubting Your Greatness and Start Living an Awesome Life 12.08\n",
      "Wildlife of New York: A Five-Borough Coloring Book 22.14\n",
      "What Happened on Beale Street (Secrets of the South Mysteries #2) 25.37\n",
      "Unreasonable Hope: Finding Faith in the God Who Brings Purpose to Your Pain 46.33\n",
      "Under the Tuscan Sun 37.33\n",
      "Toddlers Are A**holes: It's Not Your Fault 25.55\n",
      "The Year of Living Biblically: One Man's Humble Quest to Follow the Bible as Literally as Possible 34.72\n",
      "The Whale 35.96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://books.toscrape.com/catalogue/page-26.html'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Requests library is one of the integral part of Python for making HTTP requests to a specified URL\n",
    "\n",
    "Link_inicial = \"https://books.toscrape.com/catalogue/page-25.html\"\n",
    "Page_inicial  = requests.get(Link_inicial) #para acceder a la pagina\n",
    "#print(page.text)\n",
    "soup = BeautifulSoup(Page_inicial.content, 'html.parser') #convertir la pagina en una sopa\n",
    "\n",
    "html_booksnames = soup.select(\"h3 a\") #seleccionar los tag de html para acceder a lugares especificos de la pagina\n",
    "html_booksprices = soup.select(\"p.price_color\")\n",
    "\n",
    "#Books_Names = np.zeros(len(html_booksnames)).astype(str)\n",
    "#Books_Prices = np.zeros(len(html_booksnames)).astype(str)\n",
    "\n",
    "Books_Names = [x.get(\"title\") for x in html_booksnames ] #se pueden usar metodos de sopa para extraer la informacion\n",
    "\n",
    "Books_Prices = [re.findall(r\"\\d+.\\d+\",str(x))[0] for x in html_booksprices] #se pueden usar metodos de RegEx para extraer la informacion\n",
    "\n",
    "for i in range(len(Books_Names)): \n",
    "    #Books_Names[i] = html_booksnames[i].get(\"title\")\n",
    "    \n",
    "    #Books_Names[i] = re.findall('title=\\\"(.*)\\\">',str(html_booksnames[i]) )[0]\n",
    "    #Books_Prices[i] = re.findall(r\"\\d+.\\d+\",str(html_booksprices[i]))[0]\n",
    "    print(Books_Names[i],Books_Prices[i])\n",
    "\n",
    "#other form for books names\n",
    "#lista_article = soup.find_all(\"article\",class_=\"product_pod\") #otro metodo de sopa para extraer informacion especifica\n",
    "#Books_Names = [x.find(\"h3\").find(\"a\").get(\"title\") for x in lista_article]\n",
    "#Books_Names = [re.findall('title=\\\"(.*)\\\">',str(x) )[0] for x in html_booksnames] No funciono porque page 25 libro 11 empiza\n",
    "                                                                                 #con comilla simple '' y mi RegEx con \"\"\n",
    "urljoin(Link_inicial,soup.select(\"li.next > a\")[0].get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1023d91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping now ##### https://books.toscrape.com/\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-2.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-3.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-4.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-5.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-6.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-7.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-8.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-9.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-10.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-11.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-12.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-13.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-14.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-15.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-16.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-17.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-18.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-19.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-20.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-21.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-22.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-23.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-24.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-25.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-26.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-27.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-28.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-29.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-30.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-31.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-32.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-33.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-34.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-35.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-36.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-37.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-38.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-39.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-40.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-41.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-42.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-43.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-44.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-45.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-46.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-47.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-48.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-49.html\n",
      "Scrapping now ##### https://books.toscrape.com/catalogue/page-50.html\n"
     ]
    }
   ],
   "source": [
    "def Scrapp_Link(Link): #Function that scrape a link and extract specific information from the web page\n",
    "    Page_inicial  = requests.get(Link)\n",
    "    soup = BeautifulSoup(Page_inicial.content, 'html.parser')\n",
    "    \n",
    "    html_booksnames = soup.select(\"h3 a\")\n",
    "    html_booksprices = soup.select(\"p.price_color\")\n",
    "    \n",
    "    Books_Names = np.array([x.get(\"title\") for x in html_booksnames])\n",
    "    Books_Prices = np.array([re.findall(r\"\\d+.\\d+\",str(x))[0] for x in html_booksprices])\n",
    "    \n",
    "    Matrix = np.vstack((Books_Names, Books_Prices)).T #for built an array with form of columns, matrix nX2\n",
    "    \n",
    "    if not not soup.select(\"li.next > a\"):\n",
    "        Next_link = soup.select(\"li.next > a\")[0].get(\"href\") #link of next page\n",
    "        Next_link = urljoin(Link, Next_link)\n",
    "    else:\n",
    "        Next_link = \"There are no more web pages\"\n",
    "    \n",
    "    return Matrix,Next_link\n",
    "\n",
    "\n",
    "i=0\n",
    "Link= \"https://books.toscrape.com/\"\n",
    "\n",
    "while i<50:\n",
    "    i+=1\n",
    "    print(\"Scrapping now ##### \"+Link)\n",
    "    \n",
    "    Matrix,Link = Scrapp_Link(Link)\n",
    "    \n",
    "    if i==1:\n",
    "        Matrix1=Matrix\n",
    "        continue\n",
    "        \n",
    "    Matrix1=np.concatenate((Matrix1,Matrix)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4b7e69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Prices_Euros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>51.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>53.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>50.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>47.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>54.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Alice in Wonderland (Alice's Adventures in Won...</td>\n",
       "      <td>55.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Ajin: Demi-Human, Volume 1 (Ajin: Demi-Human #1)</td>\n",
       "      <td>57.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>A Spy's Devotion (The Regency Spies of London #1)</td>\n",
       "      <td>16.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1st to Die (Women's Murder Club #1)</td>\n",
       "      <td>53.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1,000 Places to See Before You Die</td>\n",
       "      <td>26.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Name  Prices_Euros\n",
       "0                                 A Light in the Attic         51.77\n",
       "1                                   Tipping the Velvet         53.74\n",
       "2                                           Soumission         50.10\n",
       "3                                        Sharp Objects         47.82\n",
       "4                Sapiens: A Brief History of Humankind         54.23\n",
       "..                                                 ...           ...\n",
       "995  Alice in Wonderland (Alice's Adventures in Won...         55.53\n",
       "996   Ajin: Demi-Human, Volume 1 (Ajin: Demi-Human #1)         57.06\n",
       "997  A Spy's Devotion (The Regency Spies of London #1)         16.97\n",
       "998                1st to Die (Women's Murder Club #1)         53.98\n",
       "999                 1,000 Places to See Before You Die         26.08\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_Books_Scrap=pd.DataFrame(data=Matrix1, columns=[\"Name\",\"Prices_Euros\"])\n",
    "Data_Books_Scrap[\"Prices_Euros\"]=Data_Books_Scrap[\"Prices_Euros\"].astype(float)\n",
    "\n",
    "Data_Books_Scrap.to_csv(\"Data_Books_Scrap\")\n",
    "Data_Books_Scrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1faa1b2",
   "metadata": {},
   "source": [
    "### Other form that also work\n",
    "def Scrapp_Link(soup): #Function that scrape a link\n",
    "    \n",
    "    \n",
    "    html_booksnames = soup.select(\"h3 a\")\n",
    "    html_booksprices = soup.select(\"p.price_color\")\n",
    "    \n",
    "    Books_Names = np.array([x.get(\"title\") for x in html_booksnames])\n",
    "    Books_Prices = np.array([float(re.findall(r\"\\d+.\\d+\",str(x))[0]) for x in html_booksprices])\n",
    "    \n",
    "    Matrix = np.vstack((Books_Names, Books_Prices)).T #for built an array with form of columns  \n",
    "    \n",
    "    return Matrix\n",
    "\n",
    "i=0\n",
    "Link= \"https://books.toscrape.com/\"\n",
    "\n",
    "while i<51:\n",
    "    i+=1\n",
    "    \n",
    "    print(\"Scrapping now ##### \"+Link)\n",
    "    \n",
    "    Page_inicial  = requests.get(Link)\n",
    "    soup = BeautifulSoup(Page_inicial.content, 'html.parser')\n",
    "    \n",
    "    Matrix = Scrapp_Link(soup)\n",
    "    \n",
    "    if i==1:\n",
    "        Matrix1=Matrix\n",
    "        Link = urljoin(Link,soup.select(\"li.next > a\")[0].get(\"href\"))\n",
    "        continue\n",
    "    \n",
    "    Matrix1=np.concatenate((Matrix1,Matrix))\n",
    "    \n",
    "    if not soup.select(\"li.next > a\"):\n",
    "        break\n",
    "    Link = urljoin(Link,soup.select(\"li.next > a\")[0].get(\"href\"))\n",
    "        \n",
    "Matrix1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705accc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
